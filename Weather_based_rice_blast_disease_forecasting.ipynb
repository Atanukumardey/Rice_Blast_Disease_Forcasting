{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import configparser\n",
    "import random\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import img_to_array, ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cv.namedWindow(\"image\", cv.WINDOW_NORMAL)\n",
    "# cv.resizeWindow(\"image\", 800, 500)\n",
    "\n",
    "# np_oldopt = np.get_printoptions()\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "#\n",
    "# file and folder names\n",
    "#\n",
    "config = configparser.ConfigParser()\n",
    "config.read('codeconfig.ini')\n",
    "\n",
    "infolder = config['path']['infolder']\n",
    "outfolder = config['path']['outfolder']\n",
    "img_out = config['path']['img_out']\n",
    "\n",
    "extensions = (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")\n",
    "\n",
    "imgoutfolder = os.path.join(outfolder + img_out.split(\"/\")[0])\n",
    "if (not os.path.exists(imgoutfolder)):\n",
    "    os.makedirs(imgoutfolder)\n",
    "\n",
    "inpath = glob.glob(infolder + \"*.*\")\n",
    "\n",
    "n_classes = int(config['classinfo']['n_classes'])\n",
    "clsnames = str(config['classinfo']['target_names']).encode('ascii', 'ignore')\n",
    "\n",
    "target_names = []\n",
    "labels = []\n",
    "\n",
    "for i in range(n_classes):\n",
    "    target_names.append(clsnames.split(', ')[i])\n",
    "    labels.append(float(i + 1))\n",
    "\n",
    "filecount = 0\n",
    "\n",
    "# flags\n",
    "preproc_f = 0\n",
    "rembg_f = 0\n",
    "seg_f = 0\n",
    "crop_f = 0\n",
    "aug_f = 1\n",
    "only_aug = 1\n",
    "\n",
    "\n",
    "#\n",
    "# MAIN CODE BEGINS\n",
    "#\n",
    "\n",
    "if(not only_aug):\n",
    "    for f in inpath:\n",
    "        name, ext = os.path.splitext(os.path.basename(f))\n",
    "\n",
    "        if (str(ext).lower() not in extensions):\n",
    "            continue\n",
    "\n",
    "        file_number = (name.split('(', 1)[1]).split(')', 1)[0]\n",
    "        filecount += 1\n",
    "\n",
    "        print (\"\\n\\t {}. processing {}\").format(filecount, name)\n",
    "\n",
    "        found = 0\n",
    "        for i in range(n_classes):\n",
    "            if (name.startswith(str(target_names[i]).lower())):\n",
    "                class_number = labels[i]\n",
    "                found = 1\n",
    "\n",
    "        if (not found):\n",
    "            print (\"Wrong class - {}\").format(class_number)\n",
    "            sys.exit(0)\n",
    "\n",
    "        #\n",
    "        # read image\n",
    "        #\n",
    "        img = im1 = cv.imread(f)\n",
    "\n",
    "        # image processing\n",
    "        if(preproc_f):\n",
    "            img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "            img_hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "\n",
    "            img_rgb_chanel = cv.split(img)\n",
    "            img_hsv_chanel = cv.split(img_hsv)\n",
    "\n",
    "        # remove background\n",
    "        if(rembg_f):\n",
    "            thresh_hsv_s = 0.28 * 255\n",
    "\n",
    "            ret_s, hsv_s_thresh = cv.threshold(\n",
    "                img_hsv_chanel[1], thresh_hsv_s, 255, cv.THRESH_BINARY)\n",
    "            hsv_s_thresh = cv.GaussianBlur(hsv_s_thresh, (5, 5), 0)\n",
    "            rembg = cv.bitwise_and(img, img, mask=hsv_s_thresh)\n",
    "\n",
    "            # cv.imwrite(outfolder + img_out + name + \"-rembg\" + \".png\", rembg)\n",
    "\n",
    "        # TODO write comments\n",
    "\n",
    "        # disease segmentation\n",
    "        if (seg_f):\n",
    "            hue = img_hsv_chanel[0]\n",
    "            thresh_hsv_h = 22\n",
    "\n",
    "            _, k = cv.threshold(hue, thresh_hsv_h, 255, cv.THRESH_BINARY)\n",
    "\n",
    "            k = cv.medianBlur(k, 15)\n",
    "            k_inv = cv.bitwise_not(k)\n",
    "\n",
    "            seg = cv.bitwise_and(rembg, rembg, mask=k_inv)\n",
    "            seg = cv.medianBlur(seg, 7)\n",
    "            # cv.imwrite(outfolder + img_out + name + \"-seg\" + \".png\", seg)\n",
    "\n",
    "            seg_gray = cv.cvtColor(seg, cv.COLOR_RGB2GRAY)\n",
    "            _, seg_bin = cv.threshold(seg_gray, 40, 255, cv.THRESH_BINARY)\n",
    "\n",
    "            seg_bin = cv.copyMakeBorder(seg_bin, 10, 10, 10, 10,\n",
    "                                        cv.BORDER_CONSTANT, (0, 0, 0))\n",
    "\n",
    "            # cv.imwrite(outfolder + img_out + name + \"-segbin\" + \".png\", seg_bin)\n",
    "\n",
    "            new_segmask = np.zeros(seg_bin.shape, np.uint8)\n",
    "\n",
    "            seg_canny = cv.Canny(seg_bin, 100, 200)\n",
    "\n",
    "            contours, hierarchy = cv.findContours(\n",
    "                seg_canny, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            if(len(contours)):\n",
    "                sorted_cnts = sorted(\n",
    "                    contours, key=cv.contourArea, reverse=True)\n",
    "                cnt_max = cv.contourArea(sorted_cnts[0])\n",
    "\n",
    "                for c, e in enumerate(contours):\n",
    "                    area = cv.contourArea(contours[c])\n",
    "                    if (area >= cnt_max*0.1):\n",
    "                        cv.drawContours(new_segmask, [contours[c]], 0, 255, -1)\n",
    "\n",
    "            rb = rembg.copy()\n",
    "            rb = cv.copyMakeBorder(rb, 10, 10, 10, 10,\n",
    "                                   cv.BORDER_CONSTANT, (0, 0, 0))\n",
    "            seg = cv.bitwise_and(rb, rb, mask=new_segmask)\n",
    "            # cv.imwrite(outfolder + img_out + name + \"-newseg\" + \".png\", seg)\n",
    "\n",
    "        # cropping images\n",
    "        if (crop_f):\n",
    "            # points are col x row\n",
    "            # images are row x col\n",
    "\n",
    "            seg_bin = new_segmask\n",
    "            segline = seg.copy()\n",
    "\n",
    "            row_y = seg_bin.shape[0]\n",
    "            col_x = seg_bin.shape[1]\n",
    "\n",
    "            left = seg_bin.shape[1]\n",
    "            p_left1 = (0, 0)\n",
    "            p_left2 = (0, 0)\n",
    "\n",
    "            for y in range(0, row_y):\n",
    "                for x in range(0, col_x):\n",
    "                    if (seg_bin[y][x]):\n",
    "                        if (x <= left):\n",
    "                            left = x\n",
    "\n",
    "            p_left1 = (left-2, 0)\n",
    "            p_left2 = (left-2, col_x-2)\n",
    "            # left_line = cv.line(segline, p_left1, p_left2, (255, 0, 0), 1)\n",
    "\n",
    "            # print(\"Left = {} to {}\").format(p_left1, p_left2)\n",
    "            # cv.imwrite(outfolder + img_out + name + \"-cropbox\" + \".png\", left_line)\n",
    "\n",
    "            top = seg_bin.shape[0]\n",
    "            p_top1 = (0, 0)\n",
    "            p_top2 = (0, 0)\n",
    "\n",
    "            for x in range(col_x-1, 0, -1):\n",
    "                for y in range(row_y-1, 0, -1):\n",
    "                    if (seg_bin[y][x]):\n",
    "                        if (y <= top):\n",
    "                            top = y\n",
    "\n",
    "            p_top1 = (0, top-2)\n",
    "            p_top2 = (col_x-2, top-2)\n",
    "            # top_line = cv.line(segline, p_top1, p_top2, (255, 0, 0), 1)\n",
    "\n",
    "            # print(\"Top = {} to {}\").format(p_top1, p_top2)\n",
    "            # cv.imwrite(outfolder + img_out + name + \"-cropbox\" + \".png\", top_line)\n",
    "\n",
    "            right = 0\n",
    "            p_right1 = (0, 0)\n",
    "            p_right2 = (0, 0)\n",
    "\n",
    "            for y in range(row_y-1, 0, -1):\n",
    "                for x in range(col_x-1, 0, -1):\n",
    "                    if (seg_bin[y][x]):\n",
    "                        if (x >= right):\n",
    "                            right = x\n",
    "\n",
    "            p_right1 = (right+2, 0)\n",
    "            p_right2 = (right+2, col_x+2)\n",
    "            # right_line = cv.line(segline, p_right1, p_right2, (255, 0, 0), 1)\n",
    "\n",
    "            # print(\"Right = {} to {}\").format(p_right1, p_right2)\n",
    "            # cv.imwrite(outfolder + img_out + name + \"-cropbox\" + \".png\", right_line)\n",
    "\n",
    "            bottom = 0\n",
    "            p_bottom1 = (0, 0)\n",
    "            p_bottom2 = (0, 0)\n",
    "\n",
    "            for x in range(0, col_x):\n",
    "                for y in range(0, row_y):\n",
    "                    if (seg_bin[y][x]):\n",
    "                        if (y >= bottom):\n",
    "                            bottom = y\n",
    "\n",
    "            p_bottom1 = (0, bottom+2)\n",
    "            p_bottom2 = (col_x+2, bottom+2)\n",
    "            # bottom_line = cv.line(segline, p_bottom1, p_bottom2, (255, 0, 0), 1)\n",
    "\n",
    "            # print(\"Bottom = {} to {}\").format(p_bottom1, p_bottom2)\n",
    "            # cv.imwrite(outfolder + img_out + name + \"-croplines\" + \".png\", bottom_line)\n",
    "\n",
    "            # intersection of lines\n",
    "            x_tl = (p_left1[0], p_top1[1])\n",
    "            x_tr = (p_right1[0], p_top2[1])\n",
    "            x_bl = (p_left2[0], p_bottom1[1])\n",
    "            x_br = (p_right2[0], p_bottom2[1])\n",
    "\n",
    "            # test the intersections\n",
    "            # bottom_line[x_bl[1]][x_bl[0]] = (255, 255, 255)\n",
    "            # cv.imwrite(outfolder + img_out + name + \"-cross\" + \".png\", bottom_line)\n",
    "\n",
    "            # print(\"Top-left point = {} \\nBottom-right point = {}\").format(x_tl, x_br)\n",
    "            # cropbox = cv.rectangle(seg, x_tl, x_br, (0,0,0), 1)\n",
    "            # cv.imwrite(outfolder + img_out + name + \"-cropbox\" + \".png\", cropbox)\n",
    "\n",
    "            cropped = seg.copy()\n",
    "            cropped = cropped[x_tl[1]:x_br[1], x_tl[0]:x_br[0]]\n",
    "            cropped = cv.copyMakeBorder(cropped, 10, 10, 10, 10,\n",
    "                                        cv.BORDER_CONSTANT, (0, 0, 0))\n",
    "            # cv.imwrite(outfolder + img_out + name + \"-cropped\" + \".png\", cropped)\n",
    "            cv.imwrite(outfolder + img_out + name + \".png\", cropped)\n",
    "\n",
    "\n",
    "# image augmentation\n",
    "if(aug_f):\n",
    "    infolder = imgoutfolder\n",
    "    inpath = glob.glob(infolder + \"/\" + \"*.*\")\n",
    "\n",
    "    data_aug = os.path.join(outfolder + \"aug/\")\n",
    "    if (not os.path.exists(data_aug)):\n",
    "        os.makedirs(data_aug)\n",
    "\n",
    "    def motion_blur(img, angle, deg):\n",
    "        rotation = cv.getRotationMatrix2D((deg / 2, deg / 2), angle, 1)\n",
    "\n",
    "        kernel = np.diag(np.ones(deg))\n",
    "        kernel = (cv.warpAffine(kernel, rotation, (deg, deg))) / deg\n",
    "\n",
    "        res = cv.filter2D(img, -1, kernel)\n",
    "        cv.normalize(res, res, 0, 255, cv.NORM_MINMAX)\n",
    "        res = np.array(res, dtype=np.uint8)\n",
    "        return res\n",
    "\n",
    "    for f in inpath:\n",
    "        name, ext = os.path.splitext(os.path.basename(f))\n",
    "\n",
    "        print(\"\\n\\tAugmenting {}\").format(name)\n",
    "\n",
    "        img_orig = cv.imread(f)\n",
    "        img = cv.resize(img_orig, (512, 512), interpolation=cv.INTER_AREA)\n",
    "        data = img_to_array(img)\n",
    "        samples = np.expand_dims(data, axis=0)\n",
    "\n",
    "        aug = ImageDataGenerator(\n",
    "            horizontal_flip=True,\n",
    "            rotation_range=90, shear_range=0.15,\n",
    "            width_shift_range=0.2, height_shift_range=0.2,\n",
    "            zoom_range=[0.5, 2], brightness_range=[0.25, 1.5],\n",
    "            fill_mode=\"nearest\")\n",
    "\n",
    "        gen = aug.flow(samples)\n",
    "\n",
    "        for i in range(25):\n",
    "            im = gen.next()\n",
    "            im = im.reshape((512, 512, 3))\n",
    "\n",
    "            # motion blur randomly, about 30%  data\n",
    "            rand = random.randint(0, 2)\n",
    "            if (not rand):\n",
    "                im = motion_blur(im, 50, 16)\n",
    "\n",
    "            cv.imwrite(data_aug + name + \"#\" + str(i) + \".png\", im)\n",
    "\n",
    "\n",
    "print \"COMPLETED\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import shutil\n",
    "import cPickle\n",
    "import configparser\n",
    "import time\n",
    "import datetime as dt\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import kurtosis, skew\n",
    "from skimage.feature import greycomatrix, greycoprops, local_binary_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# timing and timestamping functions\n",
    "genesis = time.time()  # timing\n",
    "timestamp = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "# cv.namedWindow(\"image\", cv.WINDOW_NORMAL)\n",
    "# cv.resizeWindow(\"image\", 800, 500)\n",
    "\n",
    "#np_oldopt = np.get_printoptions()\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "#\n",
    "# file and folder names\n",
    "#\n",
    "config = configparser.ConfigParser()\n",
    "config.read('codeconfig.ini')\n",
    "\n",
    "infolder = config['path']['infolder']\n",
    "outfolder = config['path']['outfolder']\n",
    "img_out = config['path']['img_out']\n",
    "\n",
    "extensions = (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgoutfolder = os.path.join(outfolder + img_out.split(\"/\")[0])\n",
    "if (not os.path.exists(imgoutfolder)):\n",
    "    os.makedirs(imgoutfolder)\n",
    "\n",
    "inpath = glob.glob(infolder + \"*.*\")\n",
    "\n",
    "fextout = os.path.join(outfolder + 'FEXT-' + timestamp)\n",
    "os.makedirs(fextout)\n",
    "\n",
    "feat_list = open(fextout + '/' + 'Feature_List-' + timestamp + '.txt', 'w')\n",
    "colout = open(fextout + '/' + 'FL-' + timestamp + '-feature columns.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FL = []\n",
    "flname = []\n",
    "flname_on = 1\n",
    "csv_idx = []\n",
    "csv_idx_on = 1\n",
    "filecount = 0\n",
    "\n",
    "n_classes = int(config['classinfo']['n_classes'])\n",
    "clsnames = str(config['classinfo']['target_names']).encode('ascii', 'ignore')\n",
    "\n",
    "target_names = []\n",
    "labels = []\n",
    "\n",
    "for i in range(n_classes):\n",
    "    target_names.append(clsnames.split(', ')[i])\n",
    "    labels.append(float(i + 1))\n",
    "\n",
    "#\n",
    "# flags\n",
    "#\n",
    "resize_f = 0\n",
    "\n",
    "color_f = 1\n",
    "shape_f = 1\n",
    "texture_f = 1\n",
    "\n",
    "mb_f = 1\n",
    "mg_f = 1\n",
    "mr_f = 1\n",
    "mh_f = 1\n",
    "ms_f = 1\n",
    "mv_f = 0\n",
    "sdb_f = 1\n",
    "sdg_f = 1\n",
    "sdr_f = 1\n",
    "bghist_f = (16 * 16)\n",
    "hshist_f = (16 * 16)  # bin * bin\n",
    "skw_f = 1\n",
    "kurt_f = 1\n",
    "sdseg_f = 1\n",
    "\n",
    "cntperi_f = 1\n",
    "cntarea_f = 1\n",
    "hum_f = 7  # 7 moments\n",
    "\n",
    "glcm_f = 4 * 1  # mean of 4 glcm, 4 angle\n",
    "lbphist_f = 32  # 32  # bins\n",
    "\n",
    "# flags finished\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# calculate feature number for feature list shaping\n",
    "prid_f_num = 1\n",
    "class_f_num = 1\n",
    "\n",
    "if (color_f):\n",
    "    color_f_num = mb_f + mg_f + mr_f + mh_f + ms_f + mv_f + \\\n",
    "        sdb_f + sdg_f + sdr_f + bghist_f + hshist_f + skw_f + kurt_f + sdseg_f\n",
    "else:\n",
    "    color_f_num = 0\n",
    "\n",
    "if (shape_f):\n",
    "    shape_f_num = cntperi_f + cntarea_f + hum_f\n",
    "else:\n",
    "    shape_f_num = 0\n",
    "\n",
    "if (texture_f):\n",
    "    texture_f_num = glcm_f + lbphist_f\n",
    "else:\n",
    "    texture_f_num = 0\n",
    "\n",
    "feat_num = prid_f_num + class_f_num + color_f_num + \\\n",
    "    shape_f_num + texture_f_num\n",
    "\n",
    "#\n",
    "# MAIN CODE BEGINS\n",
    "#\n",
    "\n",
    "print \"\\n Beginning feature extraction... \\n\"\n",
    "\n",
    "\n",
    "fext_gen = time.time()\n",
    "\n",
    "for f in inpath:\n",
    "    name, ext = os.path.splitext(os.path.basename(f))\n",
    "\n",
    "    if (str(ext).lower() not in extensions):\n",
    "        continue\n",
    "\n",
    "    file_number = (name.split('(', 1)[1]).split(')', 1)[0]\n",
    "    filecount += 1\n",
    "\n",
    "    print (\"\\n\\t {}. processing {}\").format(filecount, name)\n",
    "\n",
    "    found = 0\n",
    "    for i in range(n_classes):\n",
    "        if (name.startswith(str(target_names[i]).lower())):\n",
    "            class_number = labels[i]\n",
    "            found = 1\n",
    "\n",
    "    if (not found):\n",
    "        print (\"Wrong class - {}\").format(class_number)\n",
    "        sys.exit(0)\n",
    "\n",
    "    # processing_id = classnum(1) + file_num\n",
    "    processing_id = int(str(int(class_number)) + str(file_number))\n",
    "    FL.append(processing_id)\n",
    "\n",
    "    if flname_on:\n",
    "        flname.append(\"Processing ID\")\n",
    "    if csv_idx_on:\n",
    "        csv_idx.append(\"Processing ID\")\n",
    "\n",
    "    FL.append(class_number)\n",
    "\n",
    "    if flname_on:\n",
    "        flname.append(\"Class Label\")\n",
    "    if csv_idx_on:\n",
    "        csv_idx.append(\"Class Label\")\n",
    "\n",
    "    #\n",
    "    # read images\n",
    "    #\n",
    "    img = im1 = cv.imread(f)\n",
    "\n",
    "    if (resize_f):\n",
    "        res_row = min(512, img.shape[1])\n",
    "        res_col = min(256, img.shape[0])\n",
    "        # res_row = res_col = 512\n",
    "        img = cv.resize(im1, (res_row, res_col),\n",
    "                        interpolation=cv.INTER_AREA)\n",
    "\n",
    "    img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    img_hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "\n",
    "    img_rgb_chanel = cv.split(img)\n",
    "    img_hsv_chanel = cv.split(img_hsv)\n",
    "\n",
    "    # feature extraction\n",
    "    seg = img\n",
    "    segf = seg.flatten()\n",
    "    seg_gray = cv.cvtColor(seg, cv.COLOR_RGB2GRAY)\n",
    "\n",
    "    if (color_f):\n",
    "        print \"Extracting color features...\"\n",
    "\n",
    "        b_seg, g_seg, r_seg = cv.split(seg)\n",
    "\n",
    "        b_segf = b_seg.flatten()\n",
    "        g_segf = g_seg.flatten()\n",
    "        r_segf = r_seg.flatten()\n",
    "        mean_r_seg = mean_g_seg = mean_b_seg = 0\n",
    "\n",
    "        if (mb_f):\n",
    "            if flname_on:\n",
    "                flname.append(\"Mean of RGB-Blue of segmented image\")\n",
    "            if csv_idx_on:\n",
    "                csv_idx.append(\"Mean of RGB-Blue\")\n",
    "\n",
    "            if np.count_nonzero(b_segf) != 0:\n",
    "                mean_b_seg = sum(b_segf[np.nonzero(b_segf)]) / \\\n",
    "                    np.count_nonzero(b_segf)  # FEX mean_b_seg\n",
    "                FL.append(mean_b_seg)\n",
    "            else:\n",
    "                FL.append(0.0)\n",
    "\n",
    "        if (mg_f):\n",
    "            if flname_on:\n",
    "                flname.append(\"Mean of RGB-Green of segmented image\")\n",
    "            if csv_idx_on:\n",
    "                csv_idx.append(\"Mean of RGB-Green\")\n",
    "\n",
    "            if np.count_nonzero(g_segf) != 0:\n",
    "                mean_g_seg = sum(g_segf[np.nonzero(g_segf)]) / \\\n",
    "                    np.count_nonzero(g_segf)  # FEX mean_g_seg\n",
    "                FL.append(mean_g_seg)\n",
    "            else:\n",
    "                FL.append(0.0)\n",
    "\n",
    "        if (mr_f):\n",
    "            if flname_on:\n",
    "                flname.append(\"Mean of RGB-Red of segmented image\")\n",
    "            if csv_idx_on:\n",
    "                csv_idx.append(\"Mean of RGB-Red\")\n",
    "\n",
    "            if np.count_nonzero(r_segf) != 0:\n",
    "                mean_r_seg = sum(r_segf[np.nonzero(r_segf)]) / \\\n",
    "                    np.count_nonzero(r_segf)  # FEX mean_r_seg\n",
    "                FL.append(mean_r_seg)\n",
    "            else:\n",
    "                FL.append(0.0)\n",
    "\n",
    "        seg_hsv = cv.cvtColor(seg, cv.COLOR_RGB2HSV)\n",
    "        h_seg, s_seg, v_seg = cv.split(seg_hsv)\n",
    "\n",
    "        h_segf = h_seg.flatten()\n",
    "        s_segf = s_seg.flatten()\n",
    "        v_segf = v_seg.flatten()\n",
    "        mean_h_seg = mean_s_seg = mean_v_seg = 0\n",
    "\n",
    "        if (mh_f):\n",
    "            if flname_on:\n",
    "                flname.append(\"Mean of HSV-Hue of segmented image\")\n",
    "            if csv_idx_on:\n",
    "                csv_idx.append(\"Mean of HSV-Hue\")\n",
    "\n",
    "            if np.count_nonzero(h_segf) != 0:\n",
    "                mean_h_seg = sum(h_segf[np.nonzero(h_segf)]) / \\\n",
    "                    np.count_nonzero(h_segf)  # FEX mean_h_seg\n",
    "                FL.append(mean_h_seg)\n",
    "            else:\n",
    "                FL.append(0.0)\n",
    "\n",
    "        if (ms_f):\n",
    "            if flname_on:\n",
    "                flname.append(\"Mean of HSV-Saturation of segmented image\")\n",
    "            if csv_idx_on:\n",
    "                csv_idx.append(\"Mean of HSV-Saturation\")\n",
    "\n",
    "            if np.count_nonzero(s_segf) != 0:\n",
    "                mean_s_seg = sum(s_segf[np.nonzero(s_segf)]) / \\\n",
    "                    np.count_nonzero(s_segf)  # FEX mean_s_seg\n",
    "                FL.append(mean_s_seg)\n",
    "            else:\n",
    "                FL.append(0.0)\n",
    "\n",
    "        if (mv_f):\n",
    "            if flname_on:\n",
    "                flname.append(\"Mean of HSV-Value of segmented image\")\n",
    "            if csv_idx_on:\n",
    "                csv_idx.append(\"Mean of HSV-Value\")\n",
    "\n",
    "            if np.count_nonzero(v_segf) != 0:\n",
    "                mean_v_seg = sum(v_segf[np.nonzero(v_segf)]) / \\\n",
    "                    np.count_nonzero(v_segf)  # FEX mean_v_seg\n",
    "                FL.append(mean_v_seg)\n",
    "            else:\n",
    "                FL.append(0.0)\n",
    "\n",
    "        # standard deviation\n",
    "        if (sdb_f):\n",
    "            b_seg_std = np.std(b_segf)  # FEX b_seg_std\n",
    "            FL.append(b_seg_std)\n",
    "            if flname_on:\n",
    "                flname.append(\n",
    "                    \"Standard Deviation of RGB-Blue of segmented image\")\n",
    "            if csv_idx_on:\n",
    "                csv_idx.append(\"SD of RGB-Blue\")\n",
    "\n",
    "        if (sdg_f):\n",
    "            g_seg_std = np.std(g_segf)  # FEX g_seg_std\n",
    "            FL.append(g_seg_std)\n",
    "            if flname_on:\n",
    "                flname.append(\n",
    "                    \"Standard Deviation of RGB-Green of segmented image\")\n",
    "            if csv_idx_on:\n",
    "                csv_idx.append(\"SD of RGB-Green\")\n",
    "\n",
    "        if (sdr_f):\n",
    "            r_seg_std = np.std(r_segf)  # FEX r_seg_std\n",
    "            FL.append(r_seg_std)\n",
    "            if flname_on:\n",
    "                flname.append(\n",
    "                    \"Standard Deviation of RGB-Red of segmented image\")\n",
    "            if csv_idx_on:\n",
    "                csv_idx.append(\"SD of RGB-Red\")\n",
    "\n",
    "        if (bghist_f):\n",
    "            bghistbins = 16\n",
    "            total_bins = bghistbins * bghistbins\n",
    "            img_rgb_hist_bg, img_rgb_hist_bg_edge1, img_rgb_hist_bg_edge2 = np.histogram2d(\n",
    "                b_segf, g_segf, bins=bghistbins, range=((0, 255), (0, 255)))\n",
    "\n",
    "            img_rgb_hist_bgf = np.ravel(img_rgb_hist_bg)\n",
    "\n",
    "            # min-max normalization = (x - min)/(max - min)\n",
    "            img_rgb_hist_bg_norm = (img_rgb_hist_bgf - min(img_rgb_hist_bgf)) / (\n",
    "                max(img_rgb_hist_bgf) - min(img_rgb_hist_bgf))  # FEX img_rgb_hist_bg_norm\n",
    "\n",
    "            FL.extend(np.ravel(img_rgb_hist_bg_norm))\n",
    "            if flname_on:\n",
    "                flname.append(\n",
    "                    \"min-max normalized Blue-Green Histogram (16*16 bins)\")\n",
    "\n",
    "            # writing feature column names\n",
    "            if csv_idx_on:\n",
    "                for i in range(total_bins):\n",
    "                    csv_idx.append(\"BG-Hist-\" + str(i))\n",
    "\n",
    "        if (hshist_f):\n",
    "            hshistbins = 16\n",
    "            total_bins = hshistbins * hshistbins\n",
    "            img_hsv_hist_hs, img_hsv_hist_hs_edge1, img_hsv_hist_hs_edge2 = np.histogram2d(\n",
    "                img_hsv_chanel[0].flatten(), img_hsv_chanel[1].flatten(), bins=hshistbins, range=((0, 180), (0, 256)))\n",
    "\n",
    "            img_hsv_hist_hsf = np.ravel(img_hsv_hist_hs)\n",
    "\n",
    "            # min-max normalization = (x - min)/(max - min)\n",
    "            img_hsv_hist_hs_norm = (img_hsv_hist_hsf - min(img_hsv_hist_hsf)) / (\n",
    "                max(img_hsv_hist_hsf) - min(img_hsv_hist_hsf))  # FEX img_hsv_hist_hs_norm\n",
    "\n",
    "            FL.extend(np.ravel(img_hsv_hist_hs_norm))\n",
    "            if flname_on:\n",
    "                flname.append(\n",
    "                    \"min-max normalized Hue-Saturation Histogram (16*16 bins)\")\n",
    "\n",
    "            # writing feature column names\n",
    "            if csv_idx_on:\n",
    "                for i in range(total_bins):\n",
    "                    csv_idx.append(\"HS-Hist-\" + str(i))\n",
    "\n",
    "        # processing for contour detection\n",
    "        ret1, seg_bin = cv.threshold(seg_gray, 40, 255, cv.THRESH_BINARY)\n",
    "        seg_bin = cv.medianBlur(seg_bin, 5)\n",
    "        seg_canny = cv.Canny(seg_bin, 100, 200)\n",
    "        # cv.imwrite(outfolder+ img_out + name + \"-seg_canny\" + \".png\", seg_canny)\n",
    "\n",
    "        contours, hierarchy = cv.findContours(\n",
    "            seg_canny, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "        # seg_cnt = cv.drawContours(seg, contours, -1, (0, 0, 255), 2)\n",
    "        # cv.imwrite(outfolder + img_out + name + \"-seg_cntdraw\" + \".png\", seg_cnt)\n",
    "\n",
    "        #\n",
    "        # find contour pixel coordinates and intensity\n",
    "        #\n",
    "        cnt_intensity = []\n",
    "        cnt_coord = []\n",
    "\n",
    "        for c, e in enumerate(contours):\n",
    "            mask = np.zeros(seg_canny.shape, np.uint8)\n",
    "            cv.drawContours(mask, [contours[c]], 0, 255, -1)\n",
    "            pixelpoints = np.transpose(np.nonzero(mask))  # contour coordinates\n",
    "            cnt_coord.extend(pixelpoints)\n",
    "\n",
    "            for p, e in enumerate(pixelpoints):\n",
    "                # intensity at the contour coordinates\n",
    "                intensity = seg_gray[pixelpoints[p][0]][pixelpoints[p][1]]\n",
    "                if (intensity):\n",
    "                    cnt_intensity.append(intensity)\n",
    "\n",
    "        if (skw_f):\n",
    "            seg_skew = skew(cnt_intensity, None)  # FEX seg_skew\n",
    "            FL.append(seg_skew)\n",
    "            if flname_on:\n",
    "                flname.append(\"Skewness of diseased segment\")\n",
    "            if csv_idx_on:\n",
    "                csv_idx.append(\"Skewness\")\n",
    "\n",
    "        if (kurt_f):\n",
    "            # kurtosis (pearson)     #FEX seg_kurt\n",
    "            seg_kurt = kurtosis(cnt_intensity, None, fisher=False)\n",
    "            FL.append(seg_kurt)\n",
    "            if flname_on:\n",
    "                flname.append(\"Kurtosis of diseased segment\")\n",
    "            if csv_idx_on:\n",
    "                csv_idx.append(\"Kurtosis\")\n",
    "\n",
    "        if (sdseg_f):\n",
    "            seg_cnt_std = np.std(cnt_intensity)\n",
    "            FL.append(seg_cnt_std)\n",
    "            if flname_on:\n",
    "                flname.append(\"Standard Deviation of diseased segment\")\n",
    "            if csv_idx_on:\n",
    "                csv_idx.append(\"SD of diseased segment\")\n",
    "\n",
    "    if (shape_f):\n",
    "        print \"Extracting shape features...\"\n",
    "\n",
    "        if (cntperi_f):\n",
    "            cnt_peri = []\n",
    "\n",
    "            for c, e in enumerate(contours):\n",
    "                perimeter = cv.arcLength(contours[c], True)\n",
    "                cnt_peri.append(perimeter)\n",
    "\n",
    "            # removing contours having less than 30% perimeter of the biggest contour\n",
    "            if (np.count_nonzero(cnt_peri) != 0):\n",
    "                max_cntp = max(cnt_peri)\n",
    "            else:\n",
    "                max_cntp = 0.0\n",
    "\n",
    "            cnt_perimeter = []\n",
    "\n",
    "            for i, e in enumerate(cnt_peri):\n",
    "                if (cnt_peri[i] > (max_cntp * 0.3)):\n",
    "                    cnt_perimeter.append(cnt_peri[i])\n",
    "\n",
    "            if cnt_perimeter:\n",
    "                cnt_perimeter_avg = sum(cnt_perimeter) / len(cnt_perimeter)\n",
    "            else:\n",
    "                cnt_perimeter_avg = 0.0\n",
    "\n",
    "            FL.append(cnt_perimeter_avg)  # FEX cnt_perimeter_avg\n",
    "            if flname_on:\n",
    "                flname.append(\"Average contour perimeter\")\n",
    "            if csv_idx_on:\n",
    "                csv_idx.append(\"Average contour perimeter\")\n",
    "\n",
    "        if (cntarea_f):\n",
    "            cnt_ar = []\n",
    "\n",
    "            for c, e in enumerate(contours):\n",
    "                area = cv.contourArea(contours[c])\n",
    "                cnt_ar.append(area)\n",
    "\n",
    "            # removing contours having less than 30% area of the biggest contour\n",
    "            if (np.count_nonzero(cnt_ar) != 0):\n",
    "                max_cnt = max(cnt_ar)\n",
    "            else:\n",
    "                max_cnt = 0.0\n",
    "\n",
    "            cnt_area = []\n",
    "\n",
    "            for i, e in enumerate(cnt_ar):\n",
    "                if (cnt_ar[i] > (max_cnt * 0.3)):\n",
    "                    cnt_area.append(cnt_ar[i])\n",
    "\n",
    "            if cnt_area:\n",
    "                cnt_area_avg = sum(cnt_area) / len(cnt_area)\n",
    "            else:\n",
    "                cnt_area_avg = 0.0\n",
    "\n",
    "            FL.append(cnt_area_avg)  # FEX cnt_area_avg\n",
    "            if flname_on:\n",
    "                flname.append(\"Average contour area\")\n",
    "            if csv_idx_on:\n",
    "                csv_idx.append(\"Average contour area\")\n",
    "\n",
    "        if (hum_f):\n",
    "            mmnt = cv.moments(seg_bin)\n",
    "            hu_m = cv.HuMoments(mmnt)\n",
    "\n",
    "            FL.extend(np.ravel(hu_m))  # FEX hu_mmnt\n",
    "            if flname_on:\n",
    "                flname.append(\"Hu Moments (7)\")\n",
    "            if csv_idx_on:\n",
    "                for i in range(7):\n",
    "                    csv_idx.append(\"Hu moment-\" + str(i))\n",
    "\n",
    "    if (texture_f):\n",
    "        print \"Extracting texture features...\"\n",
    "\n",
    "        if (glcm_f):\n",
    "            # mean of 4 GLCM features in 0, 45 (np.pi / 4), 90 (np.pi / 2), 135 (3 * np.pi / 4) degrees\n",
    "            glcm_seg = greycomatrix(\n",
    "                seg_gray, [3], [0, np.pi / 4, np.pi / 2, 3 * np.pi / 4], normed=True)\n",
    "\n",
    "            contrast = greycoprops(glcm_seg, 'contrast')  # FEX contrast\n",
    "\n",
    "            contrast = contrast.mean()\n",
    "            FL.append(contrast)\n",
    "\n",
    "            # use the commented version for individual values (not mean)\n",
    "            # contrast = [contrast[0][0], contrast[0][1], contrast[0][2], contrast[0][3]]\n",
    "            # FL.extend(contrast)\n",
    "            # if flname_on: flname.append(\"GLCM [at distance=3, angles=0,45,90,135]  Contrast\")\n",
    "\n",
    "            if flname_on:\n",
    "                flname.append(\n",
    "                    \"GLCM [at distance=3, mean, angles=0,45,90,135]  Contrast\")\n",
    "            if csv_idx_on:\n",
    "                csv_idx.append(\"GLCM Contrast\")\n",
    "\n",
    "            correlation = greycoprops(\n",
    "                glcm_seg, 'correlation')  # FEX correlation\n",
    "\n",
    "            correlation = correlation.mean()\n",
    "            FL.append(correlation)\n",
    "\n",
    "            # correlation = [correlation[0][0], correlation[0][1], correlation[0][2], correlation[0][3]]\n",
    "            # FL.extend(correlation)\n",
    "\n",
    "            if flname_on:\n",
    "                flname.append(\"GLCM Correlation\")\n",
    "            if csv_idx_on:\n",
    "                csv_idx.append(\"GLCM Correlation\")\n",
    "\n",
    "            energy = greycoprops(glcm_seg, 'energy')  # FEX energy\n",
    "\n",
    "            energy = energy.mean()\n",
    "            FL.append(energy)\n",
    "\n",
    "            # energy = [energy[0][0], energy[0][1], energy[0][2], energy[0][3]]\n",
    "            # FL.extend(energy)\n",
    "\n",
    "            if flname_on:\n",
    "                flname.append(\"GLCM Energy\")\n",
    "            if csv_idx_on:\n",
    "                csv_idx.append(\"GLCM Energy\")\n",
    "\n",
    "            homogeneity = greycoprops(\n",
    "                glcm_seg, 'homogeneity')  # FEX homogeneity\n",
    "\n",
    "            homogeneity = homogeneity.mean()\n",
    "            FL.append(homogeneity)\n",
    "\n",
    "            # homogeneity = [homogeneity[0][0], homogeneity[0][1], homogeneity[0][2], homogeneity[0][3]]\n",
    "            # FL.extend(homogeneity)\n",
    "\n",
    "            if flname_on:\n",
    "                flname.append(\"GLCM Homogeneity\")\n",
    "            if csv_idx_on:\n",
    "                csv_idx.append(\"GLCM Homogeneity\")\n",
    "\n",
    "        if (lbphist_f):\n",
    "            # normalized LBP (P, R = 24, 3.0) histogram\n",
    "            # print \"Calculating normalized LBP (P,R = 24, 3.0) histogram...\"\n",
    "\n",
    "            R = 3.0\n",
    "            P = int(8 * R)\n",
    "            lbphbins = 32\n",
    "\n",
    "            lbp_seg = local_binary_pattern(seg_gray, P, R, method='uniform')\n",
    "\n",
    "            lbp_seg_hist, lbp_seg_hist_edges = np.histogram(\n",
    "                lbp_seg, bins=lbphbins, range=(0, 256))\n",
    "\n",
    "            lbp_seg_hist = lbp_seg_hist.astype(\"float\")\n",
    "            lbp_seg_hist_norm = (lbp_seg_hist - min(lbp_seg_hist)) / \\\n",
    "                (max(lbp_seg_hist) - min(lbp_seg_hist))\n",
    "\n",
    "            FL.extend(np.ravel(lbp_seg_hist_norm))\n",
    "            if flname_on:\n",
    "                flname.append(\n",
    "                    \"min-max normalized LBP (P, R = 24, 3.0) histogram (32 bins)\")\n",
    "            if csv_idx_on:\n",
    "                for i in range(lbphbins):\n",
    "                    csv_idx.append(\"LBP-Hist-\" + str(i))\n",
    "\n",
    "            # cv.imwrite(outfolder + img_out + name + \"-lbp_seg\" + \".png\", lbp_seg.astype(\"uint8\"))\n",
    "\n",
    "    # formatting feature list\n",
    "    # NaN values are replaced with 0\n",
    "    FL = [0.0 if np.isnan(x) else x for x in FL]\n",
    "#    FL = [round(x, 6) for x in FL]\n",
    "\n",
    "    if (not feat_list.closed):\n",
    "        feat_list.write(str(flname))\n",
    "        total_feature = len(flname) - 2\n",
    "\n",
    "        if (hum_f):\n",
    "            total_feature += 6\n",
    "\n",
    "        print >>feat_list, \"\\n \\n Total number of features: {}\".format(\n",
    "            total_feature)\n",
    "\n",
    "        print >>feat_list, \"\\n \\n Dataset ID: {}\".format(\n",
    "            str(config['info']['dataset']).encode('ascii', 'ignore'))\n",
    "\n",
    "        if (resize_f):\n",
    "            print >>feat_list, (\"\\n Images downsized to: 512x256 or lower\")\n",
    "\n",
    "        feat_list.close()\n",
    "        flname_on = 0\n",
    "\n",
    "    if (not colout.closed):\n",
    "        colout.write(str(csv_idx))\n",
    "        colout.close()\n",
    "        csv_idx_on = 0\n",
    "\n",
    "    # dump features modularly\n",
    "    if (filecount % 50 == 0):\n",
    "        # save in temporary file for modular processing of large number of files\n",
    "        with open(fextout + '/' + 'fltmp-' + str(filecount) + '.pkl', 'ab+') as tmpfile:\n",
    "            cPickle.dump(FL, tmpfile, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        FL = []\n",
    "\n",
    "    # print (\"Length of FL (feature vector) = {}\").format(len(FL))\n",
    "\n",
    "# dump the rest of the features\n",
    "tmpfile = open(fextout + '/' + 'fltmp-' + str(filecount + 1) + '.pkl', 'ab+')\n",
    "cPickle.dump(FL, tmpfile, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "tmpfile.close()\n",
    "\n",
    "# print (\"\\n Number of feature column = {}\").format(feat_num)\n",
    "\n",
    "# concateate all the temprorary feature files\n",
    "tmppath = glob.glob(fextout + \"/\" + \"*.*\")\n",
    "FL = []\n",
    "for f in tmppath:\n",
    "    name, ext = os.path.splitext(os.path.basename(f))\n",
    "    if (str(name).startswith(\"fltmp-\")):\n",
    "        with open(f, \"rb\") as ffile:\n",
    "            FL.extend(cPickle.load(ffile))\n",
    "\n",
    "        os.remove(f)\n",
    "\n",
    "feat = np.reshape(FL, (-1, int(feat_num)))\n",
    "\n",
    "df = pd.DataFrame(feat, columns=csv_idx)\n",
    "with open(fextout + '/' + 'FL-' + timestamp + '.csv', 'w') as csvout:\n",
    "    df.to_csv(csvout, sep=',', mode='w', decimal='.')\n",
    "\n",
    "with open(fextout + '/' + 'FL-' + timestamp + '.pkl', 'wb') as outfile:\n",
    "    cPickle.dump(feat, outfile, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "outfile.close()\n",
    "\n",
    "# remove previous files\n",
    "\n",
    "remfile = glob.glob(outfolder + '*.*')\n",
    "for f in remfile:\n",
    "    os.remove(f)\n",
    "\n",
    "with open(outfolder + 'feature columns.txt', 'w') as colout:\n",
    "    colout.write(str(csv_idx))\n",
    "\n",
    "\n",
    "shutil.copy2(fextout + '/' + 'FL-' + timestamp + '.pkl',\n",
    "             outfolder + 'FL-' + timestamp + '.pkl')\n",
    "shutil.copy2(fextout + '/' + 'Feature_List-' + timestamp + '.txt',\n",
    "             outfolder + 'Feature_List-' + timestamp + '.txt')\n",
    "\n",
    "# cleanup\n",
    "\n",
    "fext_time = time.time() - fext_gen  # timing finished\n",
    "print '\\n Feature extraction completed in::: %.2fs.' % fext_time\n",
    "\n",
    "# cv.waitKey(0)\n",
    "# cv.destroyAllWindows()\n",
    "\n",
    "armageddon = time.time() - genesis  # timing finished\n",
    "\n",
    "print '\\n Total time: %.3fs.' % armageddon\n",
    "print \"\\n END OF CODE\"\n",
    "\n",
    "# run classifier\n",
    "#execfile('ms-thesis -CLF.py')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import cPickle\n",
    "import configparser\n",
    "import random\n",
    "import time\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from matplotlib import cm\n",
    "from matplotlib import pyplot as plt\n",
    "from prettytable import PrettyTable as prt\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score, StratifiedKFold, RepeatedStratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# timing and timestamping functions\n",
    "genesis = time.time()\n",
    "timestamp = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "#\n",
    "# file and folder names\n",
    "#\n",
    "config = configparser.ConfigParser()\n",
    "config.read('codeconfig.ini')\n",
    "\n",
    "infolder = config['path']['infolder']\n",
    "outfolder = config['path']['outfolder']\n",
    "\n",
    "feat_fname = glob.glob(outfolder + 'FL-*.pkl')\n",
    "\n",
    "# process the filename string of feature file\n",
    "#\n",
    "feat_fname = str(feat_fname[0])\n",
    "feat_fname = feat_fname.split('\\\\', 1)[1]\n",
    "feat_file = feat_fname.split('.', 1)[0]\n",
    "\n",
    "with open(outfolder + feat_fname, 'rb') as infile:\n",
    "    feat_in = cPickle.load(infile)\n",
    "\n",
    "with open(outfolder + 'feature columns.txt', \"r\") as inp:\n",
    "    fcols = inp.read()\n",
    "\n",
    "# matplotlib plotting global parameters\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('text', usetex=True)\n",
    "\n",
    "#\n",
    "# flags\n",
    "#\n",
    "svm_f = 0\n",
    "xgb_f = 1\n",
    "\n",
    "# global variable for comparison\n",
    "SVM_ACC = 0.0\n",
    "SVM_CV = 0.0\n",
    "XGB_ACC = 0.0\n",
    "XGB_CV = 0.0\n",
    "\n",
    "#\n",
    "# class info\n",
    "#\n",
    "n_classes = int(config['classinfo']['n_classes'])\n",
    "clsnames = str(config['classinfo']['target_names']).encode('ascii', 'ignore')\n",
    "\n",
    "target_names = []\n",
    "labels = []\n",
    "\n",
    "for i in range(n_classes):\n",
    "    target_names.append(clsnames.split(', ')[i])\n",
    "    labels.append(float(i + 1))\n",
    "\n",
    "\n",
    "# seed -> score\n",
    "# 7487 -> 91.67, cv90\n",
    "\n",
    "#seed_rand = 7487\n",
    "seed_rand = random.randint(0, 9999)\n",
    "\n",
    "test_split = 0.2\n",
    "\n",
    "fold = 10\n",
    "repeat = 1\n",
    "\n",
    "X = feat_in[..., 2:]\n",
    "y, processing_id = feat_in[..., 1], feat_in[..., 0]\n",
    "\n",
    "# processing_id = classnum + file_num\n",
    "X_train, X_test, y_train, y_test, prid_train, prid_test = train_test_split(\n",
    "    X, y, processing_id, test_size=test_split, random_state=seed_rand, stratify=y)\n",
    "\n",
    "prid_test_files = []\n",
    "test_class_list = []\n",
    "\n",
    "#\n",
    "# process processing_id string to extract class and file number\n",
    "#\n",
    "for i, e in enumerate(prid_test):\n",
    "    prid = int(prid_test[i])\n",
    "    prid_str = str(prid)\n",
    "    class_number = prid_str[0]\n",
    "    test_class_list.append(class_number)\n",
    "    file_number = prid_str[1:]\n",
    "\n",
    "    found = 0\n",
    "    for i in range(n_classes):\n",
    "        if (class_number == str(int(labels[i]))):\n",
    "            class_name = target_names[i]\n",
    "            found = 1\n",
    "\n",
    "    if (not found):\n",
    "        print (\"Wrong class - {}\").format(class_number)\n",
    "        sys.exit(0)\n",
    "\n",
    "    filename = class_name + \" (\" + file_number + \")\"\n",
    "    prid_test_files.append(filename.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_clf(clfload, clf_name, feat_file, clf, clfrep, X, y, X_train, y_train, X_test, y_test, y_pred):\n",
    "    #\n",
    "    # method for classifier evaluation and reporting\n",
    "    #\n",
    "    clf_report = metrics.classification_report(\n",
    "        y_test, y_pred, labels=labels, target_names=target_names)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    conf = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print (\"\\n Feature file ::: \\n {}\").format(feat_file)\n",
    "    print >>clfrep, (\"\\n Feature file ::: \\n {}\").format(feat_file)\n",
    "\n",
    "    print (\"\\n \\n Classifier ::: \\n {}\").format(clf)\n",
    "    print >>clfrep, (\"\\n \\n Classifier ::: \\n {}\").format(clf)\n",
    "\n",
    "    print (\"\\n \\n Classification report ::: \\n{}\").format(clf_report)\n",
    "    print >>clfrep, (\"\\n \\n Classification report ::: \\n{}\").format(clf_report)\n",
    "\n",
    "    table = prt([\"Seed\", \"Test split\"])\n",
    "    table.add_row([seed_rand, test_split])\n",
    "\n",
    "    print (\"\\n \\n {}\").format(table)\n",
    "    print >>clfrep, (\"\\n \\n {}\").format(table)\n",
    "\n",
    "    table = prt([\"Accuracy (%)\"])\n",
    "    table.add_row([round(accuracy, 4) * 100])\n",
    "\n",
    "    print (\"\\n \\n {}\").format(table)\n",
    "    print >>clfrep, (\"\\n \\n {}\").format(table)\n",
    "\n",
    "    print (\"\\n \\n Confusion matrix ::: \\n {} \\n\").format(conf)\n",
    "    print >>clfrep, (\"\\n \\n Confusion matrix ::: \\n {} \\n\").format(conf)\n",
    "\n",
    "    #\n",
    "    # map test files to prediction\n",
    "    #\n",
    "    table = prt([\"Test file\", \"Real class\", \"Predicted class\"])\n",
    "    table.align[\"Test file\"] = \"l\"\n",
    "    table.align[\"Real class\"] = \"c\"\n",
    "    table.align[\"Predicted class\"] = \"c\"\n",
    "\n",
    "    for i, e in enumerate(X_test):\n",
    "        testfile = prid_test_files[i]\n",
    "        realclass = target_names[int(test_class_list[i]) - 1]\n",
    "        predclass = target_names[int(y_pred[i]) - 1]\n",
    "\n",
    "        # marking misclassified data\n",
    "        if (realclass != predclass):\n",
    "            table.add_row([testfile, realclass, predclass])\n",
    "\n",
    "    print (\"\\n Misclassified samples :::\")\n",
    "    print >>clfrep, (\"\\n Misclassified samples :::\")\n",
    "\n",
    "    print (\"\\n {}\").format(table.get_string(sortby=\"Test file\"))\n",
    "    print >>clfrep, (\"\\n {}\").format(table.get_string(sortby=\"Test file\"))\n",
    "\n",
    "    # print SVM best parameters from GridSearchCV\n",
    "    svm_best_param = 0\n",
    "    if (clf_name == \"SVM-RBF-\"):\n",
    "        if not clfload:\n",
    "            svm_best_score = clf.best_score_\n",
    "            svm_best_param = clf.best_params_\n",
    "\n",
    "            print(\"\\n \\n Best: {} using {}\").format(\n",
    "                svm_best_score, svm_best_param)\n",
    "            print >>clfrep, (\"\\n \\n Best: {} using {}\").format(\n",
    "                round(svm_best_score, 5), svm_best_param)\n",
    "    #\n",
    "    # classwise accuracy calculation\n",
    "    #\n",
    "    cmat, classwise_acc, accu, clf_new = eval_classwise(\n",
    "        clf_name, feat_file, clf, clfrep, X, y, svm_best_param)\n",
    "\n",
    "    table = prt([\"Class\", \"Accuracy (%)\"])\n",
    "    table.align[\"Class\"] = \"l\"\n",
    "    table.align[\"Accuracy (%)\"] = \"c\"\n",
    "\n",
    "    classwise_acc_avg = []\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        cls_i_acc_avg = round(classwise_acc[i], 4) * 100\n",
    "        classwise_acc_avg.append(cls_i_acc_avg)\n",
    "        table.add_row([target_names[i], cls_i_acc_avg])\n",
    "\n",
    "    print (\"\\n {}\").format(table)\n",
    "    print >>clfrep, (\"\\n {}\").format(table)\n",
    "\n",
    "    #\n",
    "    # plot classwise accuracy\n",
    "    #\n",
    "    x_pos = np.arange(len(target_names))\n",
    "    y_pos = range(0, 105, 5)\n",
    "    perf = classwise_acc_avg\n",
    "\n",
    "    my_cmap = cm.get_cmap('Blues')\n",
    "    plt.clf()\n",
    "    plt.bar(x_pos, perf, align='center', width=0.2, color=my_cmap(perf))\n",
    "\n",
    "    plt.ylabel('Average accuracy')\n",
    "    plt.xlabel('Class')\n",
    "    plt.xticks(x_pos, target_names)\n",
    "    plt.yticks(y_pos, y_pos)\n",
    "    plt.title('Class-wise accuracy rate')\n",
    "    plt.plot()\n",
    "    plt.savefig(clfout + '/' + 'CLF-' + timestamp + '-classwise accuracy.pdf')\n",
    "\n",
    "    #\n",
    "    # cross validate with new optimized classifier\n",
    "    #\n",
    "    clf = clf_new\n",
    "\n",
    "    cv_score = cross_val_score(\n",
    "        clf, X, y, cv=RepeatedStratifiedKFold(fold, repeat))\n",
    "\n",
    "    table = prt([\"Mean of \" + str(repeat) + \" times \" + str(fold) +\n",
    "                 \"-fold CV accuracy (%)\"])\n",
    "    table.add_row([round(cv_score.mean(), 4) * 100])\n",
    "\n",
    "    print (\"\\n \\n {}\").format(table)\n",
    "    print >>clfrep, (\"\\n \\n {}\").format(table)\n",
    "\n",
    "    # cleanup\n",
    "    clfrep.close()\n",
    "    with open(clfout + '/' + 'CLF-' + timestamp + '.pkl', 'wb') as outfile:\n",
    "        cPickle.dump(clf, outfile, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    clf_time = time.time() - clf_gen  # timing finished\n",
    "\n",
    "    print '\\n Classification finished in::: %.2fs.' % clf_time\n",
    "\n",
    "    return [(round(accuracy, 4) * 100), (round(cv_score.mean(), 4) * 100)]\n",
    "    # method finished\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_classwise(clf_name, feat_file, clf, clfrep, X, y, svm_best_param):\n",
    "    cmat = []\n",
    "    accu = []\n",
    "\n",
    "    # new SVM classifier with optimized parameters\n",
    "    if (clf_name == \"SVM-RBF-\"):\n",
    "        if not clfload:\n",
    "            old_clf = clf\n",
    "            clf = svm.SVC(C=svm_best_param[\"C\"], gamma='scale', kernel='rbf')\n",
    "\n",
    "    #\n",
    "    # manual m-times k-fold CV\n",
    "    #\n",
    "    for i in range(repeat):\n",
    "        # random.seed(i**2)\n",
    "        #seed_rand = random.randint(0, 999)\n",
    "\n",
    "        skf = StratifiedKFold(fold, shuffle=True, random_state=seed_rand)\n",
    "\n",
    "        for train_idx, test_idx in skf.split(X, y):\n",
    "            X_train = X[train_idx]\n",
    "            X_test = X[test_idx]\n",
    "            y_train = y[train_idx]\n",
    "            y_test = y[test_idx]\n",
    "\n",
    "            clf = clf.fit(X_train, y_train)\n",
    "\n",
    "            # new XGB prediction with optimized parameters\n",
    "            if (clf_name == \"XGB-\"):\n",
    "                y_pred = clf.predict(X_test, ntree_limit=clf.best_ntree_limit)\n",
    "            else:\n",
    "                y_pred = clf.predict(X_test)\n",
    "\n",
    "            acc = metrics.accuracy_score(y_test, y_pred, normalize=True)\n",
    "            accu.append(acc)\n",
    "\n",
    "            # calculating confusion matrix every round\n",
    "            conf = metrics.confusion_matrix(y_test, y_pred)\n",
    "            cm = conf.astype(\"float\").diagonal() / conf.sum(axis=1)\n",
    "            cmat.append(cm)\n",
    "\n",
    "    cmat = np.ravel(cmat)\n",
    "    cmat = np.reshape(cmat, (-1, n_classes))\n",
    "\n",
    "    classwise_acc = []\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        cls_i_acc = np.array(cmat)[:, i]\n",
    "        cls_i_acc_avg = sum(cls_i_acc) / len(cls_i_acc)\n",
    "        classwise_acc.append(cls_i_acc_avg)\n",
    "\n",
    "    return cmat, classwise_acc, accu, clf\n",
    "\n",
    "\n",
    "if (svm_f):\n",
    "    print \"\\n Beginning image classification... \\n\"\n",
    "    clf_gen = time.time()\n",
    "\n",
    "    clf_name = \"SVM-RBF-\"\n",
    "\n",
    "    clfout = os.path.join(outfolder + 'CLF-' + clf_name + timestamp)\n",
    "    os.makedirs(clfout)\n",
    "\n",
    "    clfrep = open(clfout + '/' + 'Classification_Report-' +\n",
    "                  timestamp + '.txt', 'w')\n",
    "\n",
    "    # SVM\n",
    "    clfload = 0\n",
    "    if clfload:\n",
    "        # load classifier\n",
    "        clfpath = glob.glob(\n",
    "            outfolder + \"CLF-SVM-RBF-2020-01-01_00-00-06/CLF-2020-01-01_00-00-06.pkl\")\n",
    "\n",
    "        clfpath = str(clfpath[0])\n",
    "        with open(clfpath, \"rb\") as clfin:\n",
    "            clf = cPickle.load(clfin)\n",
    "    else:\n",
    "        # create classifier\n",
    "        # gamma ='scale' => 1 / (n_features * X.var())\n",
    "        svc = svm.SVC(gamma='scale', kernel='rbf', cache_size=100)\n",
    "        # parameter tuning\n",
    "        param_grid = [{'C': [2**9, 2**10, 2**15, 2**20]}]\n",
    "        clf = GridSearchCV(svc, param_grid,\n",
    "                           cv=StratifiedKFold(fold), iid=False, verbose=0, error_score=np.nan)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    SVM_ACC, SVM_CV = evaluate_clf(clfload, clf_name, feat_file,\n",
    "                                   clf, clfrep, X, y, X_train, y_train, X_test, y_test, y_pred)\n",
    "\n",
    "\n",
    "if (xgb_f):\n",
    "    print \"\\n Beginning image classification... \\n\"\n",
    "    clf_gen = time.time()\n",
    "\n",
    "    clf_name = \"XGB-\"\n",
    "\n",
    "    clfout = os.path.join(outfolder + 'CLF-' + clf_name + timestamp)\n",
    "    os.makedirs(clfout)\n",
    "\n",
    "    clfrep = open(clfout + '/' + 'Classification_Report-' +\n",
    "                  timestamp + '.txt', 'w')\n",
    "\n",
    "    # xGBoost\n",
    "    clfload = 0\n",
    "    if clfload:\n",
    "        # load classifier\n",
    "        clfpath = glob.glob(\n",
    "            outfolder + \"CLF-XGB-2020-01-01_00-00-06/CLF-2020-01-01_00-00-06.pkl\")\n",
    "\n",
    "        clfpath = str(clfpath[0])\n",
    "        with open(clfpath, \"rb\") as clfin:\n",
    "            clf = cPickle.load(clfin)\n",
    "    else:\n",
    "        clf = xgb.XGBClassifier(min_child_weight=5)\n",
    "\n",
    "    eval_set = [(X_test, y_test)]\n",
    "    clf = clf.fit(X_train, y_train, early_stopping_rounds=10,\n",
    "                  eval_metric=\"mlogloss\", eval_set=eval_set, verbose=0)\n",
    "\n",
    "    y_pred = clf.predict(X_test, ntree_limit=clf.best_ntree_limit)\n",
    "\n",
    "    #\n",
    "    # extracting feature columns (from file)\n",
    "    #\n",
    "    # processing file name string\n",
    "    fcols = fcols.replace('\\'', '')\n",
    "    fcols = fcols.replace('[', '')\n",
    "    fcols = fcols.replace(']', '')\n",
    "\n",
    "    fc = []\n",
    "    for i in range(feat_in.shape[1]):\n",
    "        a = fcols.split(', ')[i]\n",
    "        fc.append(a)\n",
    "\n",
    "    fcols = fc[2:]  # cut out first two columns (prid, class)\n",
    "\n",
    "    impr = clf.feature_importances_\n",
    "    impr = np.array(impr)\n",
    "    # find largest n features\n",
    "    impr_sort = impr.argsort()[-20:][::-1]\n",
    "\n",
    "    imp_feat = []\n",
    "    # map important features to name\n",
    "    for i, e in enumerate(impr_sort):\n",
    "        imp_feat.append(fcols[impr_sort[i]])\n",
    "\n",
    "    imp_f = imp_feat[::-1]  # reverse the list (descending order)\n",
    "\n",
    "    # dump the XGB model\n",
    "    clf.get_booster().dump_model(clfout + '/' + 'CLF-' + timestamp +\n",
    "                                 '-xgb-dump.txt', with_stats=True, dump_format='text')\n",
    "\n",
    "    # plot feature importance by gain\n",
    "    plt.clf()\n",
    "    xgb.plot_importance(clf, max_num_features=20,\n",
    "                        importance_type='gain', show_values=False, grid=False)\n",
    "\n",
    "    plt.yticks(range(len(imp_feat)), imp_f, rotation=45)\n",
    "    plt.tick_params(axis='x', which='major', labelsize=8)\n",
    "    plt.tick_params(axis='y', which='major', labelsize=4)\n",
    "    plt.xlabel(\"Gain\")\n",
    "    plt.ylabel(\"Features\")\n",
    "    plt.plot()\n",
    "    plt.savefig(clfout + '/' + 'CLF-' + timestamp + '-feature importance.pdf')\n",
    "\n",
    "    XGB_ACC, XGB_CV = evaluate_clf(clfload, clf_name, feat_file,\n",
    "                                   clf, clfrep, X, y, X_train, y_train, X_test, y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# final report\n",
    "print (\"\\n \\n Summing up...\")\n",
    "\n",
    "table = prt([\"*****\", \"Support Vector Machine\", \"Extreme Gradient Boosting\"])\n",
    "\n",
    "table.add_row([\"Accuracy\", SVM_ACC, XGB_ACC])\n",
    "table.add_row([str(fold) + \"-fold CV\", SVM_CV, XGB_CV])\n",
    "print (\"\\n \\n {}\").format(table)\n",
    "\n",
    "\n",
    "# finishing up\n",
    "armageddon = time.time() - genesis  # timing finished\n",
    "\n",
    "print '\\n Total time: %.3fs.' % armageddon\n",
    "print \"\\n END OF CODE\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
